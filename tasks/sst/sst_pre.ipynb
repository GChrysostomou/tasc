{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-17 17:20:34--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 789539 (771K) [application/zip]\n",
      "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
      "\n",
      "trainDevTestTrees_P 100%[===================>] 771.03K   426KB/s    in 1.8s    \n",
      "\n",
      "2020-02-17 17:20:36 (426 KB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
      "\n",
      "--2020-02-17 17:20:36--  http://./\n",
      "Resolving . (.)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘.’\n",
      "FINISHED --2020-02-17 17:20:36--\n",
      "Total wall clock time: 2.4s\n",
      "Downloaded: 1 files, 771K in 1.8s (426 KB/s)\n",
      "Archive:  trainDevTestTrees_PTB.zip\n",
      "   creating: trees/\n",
      "  inflating: trees/dev.txt           \n",
      "  inflating: trees/test.txt          \n",
      "  inflating: trees/train.txt         \n"
     ]
    }
   ],
   "source": [
    "!wget -nc --no-check-certificate https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip .\n",
    "!unzip trainDevTestTrees_PTB.zip\n",
    "!mv trees/* .\n",
    "!rm -r trees/\n",
    "!rm *.zip\n",
    "!touch __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.txt', 'dev.txt', 'test.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"*.txt\")\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n",
      "872\n",
      "1821\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "a = nltk.corpus.BracketParseCorpusReader(\"\", \"(train|dev|test)\\.txt\")\n",
    "\n",
    "text = {}\n",
    "labels = {}\n",
    "keys = ['train', 'dev', 'test']\n",
    "for k in keys :\n",
    "    text[k] = [x.leaves() for x in a.parsed_sents(k+'.txt') if x.label() != '2']\n",
    "    labels[k] = [int(x.label()) for x in a.parsed_sents(k+'.txt') if x.label() != '2']\n",
    "    print(len(text[k]))\n",
    "    \n",
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "import re\n",
    "\n",
    "def tokenize(text) :\n",
    "    text = \" \".join(text)\n",
    "    text = text.replace(\"-LRB-\", '')\n",
    "    text = text.replace(\"-RRB-\", \" \")\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    tokens = \" \".join([t.text.lower() for t in nlp(text)])\n",
    "    return tokens\n",
    "\n",
    "for k in keys :\n",
    "    text[k] = [tokenize(t) for t in text[k]]    \n",
    "    labels[k] = [1 if x >= 3 else 0 for x in labels[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_texts = []\n",
    "df_labels = []\n",
    "df_exp_split = []\n",
    "\n",
    "for k in keys :\n",
    "    df_texts += text[k]\n",
    "    df_labels += labels[k]\n",
    "    df_exp_split += [k]*len(text[k])\n",
    "    \n",
    "df = pd.DataFrame({'text' : df_texts, 'label' : df_labels, 'exp_split' : df_exp_split}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess_bc import cleaner\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join(cleaner(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_split</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>the rock is destined to be the qqq century s n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>singer composer bryan adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>yet the act is still charming here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>whether or not you re enlightened by any of de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>just the labour involved in creating the layer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>part of the charm of satin rouge is that it av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>a screenplay more ingeniously constructed than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>extreme ops exceeds expectations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>good fun good action good acting good dialogue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exp_split  label                                               text\n",
       "0     train      1  the rock is destined to be the qqq century s n...\n",
       "1     train      1  the gorgeously elaborate continuation of the l...\n",
       "2     train      1  singer composer bryan adams contributes a slew...\n",
       "3     train      1                 yet the act is still charming here\n",
       "4     train      1  whether or not you re enlightened by any of de...\n",
       "5     train      1  just the labour involved in creating the layer...\n",
       "6     train      1  part of the charm of satin rouge is that it av...\n",
       "7     train      1  a screenplay more ingeniously constructed than...\n",
       "8     train      1                   extreme ops exceeds expectations\n",
       "9     train      1  good fun good action good acting good dialogue..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('sst_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess_bc import extract_vocabulary_\n",
    "\n",
    "word_to_ix = extract_vocabulary_(min_df = 1, dataframe = df)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: (\"<SOS> \" + x + \" <EOS>\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess_bc import text_to_seq\n",
    "\n",
    "train_ix = text_to_seq(df[df.exp_split == \"train\"][[\"text\",\"label\"]].values, word_to_ix)\n",
    "dev_ix = text_to_seq(df[df.exp_split == \"dev\"][[\"text\",\"label\"]].values, word_to_ix)\n",
    "test_ix = text_to_seq(df[df.exp_split == \"test\"][[\"text\",\"label\"]].values, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_word = {v:k for k,v in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  11174  words out of  13686\n"
     ]
    }
   ],
   "source": [
    "from preprocess_bc import pretrained_embeds, DataHolder_BC\n",
    "\n",
    "pre = pretrained_embeds(\"fasttext.simple.300d\", ix_to_word)\n",
    "\n",
    "pretrained = pre.processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataHolder_BC(train_ix, dev_ix, test_ix, word_to_ix, embeds =  pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(data, open(\"data.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
